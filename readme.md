# The Data Librarian

**The Data Librarian** is a comprehensive Python-based suite designed to automate the organization, cleanup, and processing of massive digital archives. Just as a traditional librarian manages a physical collection, this tool helps you curate libraries of research papers, e-books, PDFs, and media files that would otherwise take hours to process by hand.

Whether you are preparing a dataset for AI tools like NotebookLM, cleaning up a sprawling backup drive, or organizing thousands of research documents, The Data Librarian provides a simple, unified web interface to handle the heavy lifting.

## Key Features

*   **Duplicate Detection & Cleanup**: Scans thousands of files to identify exact duplicates using SHA256 hashing. Instead of deleting them immediately, it safely moves them to a "Holding Bin" for your review.
*   **Intelligent PDF Splitter**: Automatically detects large PDF files (e.g., >200MB) and splits them into smaller, manageable chunks. This is essential for compatibility with tools that have strict file size limits.
*   **Web-Based Control**: No complex CLI commands to memorize. Launch the tool once and control everything from a modern, dark-mode web dashboard.
*   **Non-Destructive Operations**: The tool is built with safety first. It never overwrites your original files and always creates copies or moves files to safe locations.

---

## Getting Started

### 1. Prerequisites
You need **Python 3.x** installed on your system.
*   **Windows**: Download from [python.org](https://www.python.org/downloads/). Ensure you check **"Add Python to PATH"** during installation.
*   **Linux/Mac**: Open a terminal and run `python3 --version`. If not found, install via your package manager (e.g., `sudo apt install python3`).
*   **Node.js**: Install Node.js (v18 or newer) to run the Next.js frontend.

### 2. Installation

#### Frontend (Next.js)
1.  **Install Dependencies**:
    ```bash
    npm install
    ```
2.  **Start Development Server**:
    ```bash
    npm run dev
    ```
    The web interface will be available at [http://localhost:3000](http://localhost:3000).

#### Backend (Python Core)
1.  **Navigate to Core**:
    ```bash
    cd python_core
    ```
2.  **Install Dependencies**:
    ```bash
    pip install pypdf
    ```

### 3. Configuration (`config.json`)
Before running the tool, you can customize its behavior by editing the `config.json` file. This file uses JSON format and includes section headers for easier navigation.

#### Core Settings (`__CORE_SETTINGS__`)
| Setting | Default | Description |
| :--- | :--- | :--- |
| `PORT` | `2226` | Port for the web server. |
| `MOVE_DUPLICATES` | `false` | **Important**: Set to `true` to actually move files. If `false`, it only logs what *would* happen. |

#### Weeding Settings (`__WEEDING_SETTINGS__`)
| Setting | Default | Description |
| :--- | :--- | :--- |
| `EXCLUDED_FOLDERS` | `["_DuplicateHoldingBin"]` | List of folder names to ignore during scans. |
| `DUPLICATE_HOLDING_DIR` | `"./_DuplicateHoldingBin"` | Name of the folder where duplicates are moved. Relative paths start with `./`. |
| `LOG_NAME_PREFIX` | `"_duplicate_log"` | Prefix for the log file generated by the cleaner. |

#### Segmenting Settings (`__SEGMENTING_SETTINGS__`)
| Setting | Default | Description |
| :--- | :--- | :--- |
| `PDF_TARGET_CHUNK_MB` | `100` | The goal size for each split part. |
| `PDF_PAGE_CHUNK_LIMIT` | `1000` | Initial guess for pages per chunk. |

---

## Using the Web Interface

To start the application, open your terminal in the `data-librarian` folder and run:

**On Windows:**
```bash
python web_interface.py
```

**On Linux / Mac / WSL:**
```bash
python3 web_interface.py
```

Once running, open your web browser and navigate to:
**`http://localhost:2226`**

### The Console Log
The right-hand panel of the interface acts as your **Operations Console**. behavior:
*   **Real-Time Feedback**: It displays exactly what the script is doing (files found, moved, or created) in real-time.
*   **Log Files**: For the Duplicate Cleaner, a permanent log file (`.txt`) is saved in the `_DuplicateHoldingBin` folder for every run, so you have a permanent record of what was moved.

---

## Feature 1: Duplicate File Cleaner

**Best for:** Cleaning up unorganized backups, merging folders, and freeing up storage space.

1.  **Select Tab**: Click the **"Cleaner"** tab.
2.  **Target Folder**: Paste the absolute path of the folder you want to scan.
    *   *Default:* The folder where the script is running.
3.  **Start**: Click **"Clean Duplicates"**.
4.  **Review**:
    *   The tool calculates hashes for all files.
    *   If a duplicate is found, the *second* copy is moved to `_DuplicateHoldingBin`.
    *   **Note**: The original file is left untouched in its original location.

---

## Feature 2: PDF Splitter

**Best for:** Preparing large libraries of e-books or whitepapers for AI analysis (e.g., NotebookLM) or email sharing.

1.  **Select Tab**: Click the **"PDF Splitter"** tab.
2.  **Target Folder**: Paste the full path of the folder containing your PDF library.
    *   *Tip*: You can copy the path from your file explorer.
3.  **Customize (Optional)**:
    *   Adjust **Max File Size** if you have specific limits (e.g., NotebookLM's 200MB limit).
    *   The **Initial Page Count** is just a starting guess. The script uses an **Adaptive Algorithm**: if it splits a file and a chunk is still too big, it automatically deletes the temp file, recalculates a safer page count, and retries.
4.  **Start**: Click **"Start Splitting"**.
5.  **Result**:
    *   Large files (e.g., `MyBook.pdf`) remain untouched.
    *   New files are created alongside them: `MyBook_pages_1-1000.pdf`, `MyBook_pages_1001-2000.pdf`, etc.

---

## Troubleshooting

*   **Browser Usage**: The tool does not open the browser automatically (to support server environments). You must manually open `http://localhost:2226`.
*   **"Permission Denied"**: Ensure you have read/write access to the folders you are scanning.
*   **Cloud Folders**: If scanning a synchronized folder (OneDrive, iCloud), pause syncing first to avoid file locking errors.